{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division # Use a//b to do integer division\n",
    "from __future__ import print_function # Makes print a function, use print(\"Hello World\")\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yo\n"
     ]
    }
   ],
   "source": [
    "print(\"yo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# Get labeled data\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<type 'numpy.ndarray'>\n",
      "(2, 784)\n",
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "<type 'numpy.ndarray'>\n",
      "(2, 10)\n",
      "[[ 0.  0.  0.  0.  0.  1.  0.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "# Data exploration\n",
    "# Use mnist.train.next_batch to get a batch of data\n",
    "x_explore, y_explore = mnist.train.next_batch(2)\n",
    "print(type(x_explore))\n",
    "print(x_explore.shape)\n",
    "print(x_explore)\n",
    "print(type(y_explore))\n",
    "print(y_explore.shape)\n",
    "print(y_explore)\n",
    "\n",
    "n_features = x_explore.shape[1]\n",
    "n_output = y_explore.shape[1]\n",
    "\n",
    "x_explore /= np.max(x_explore)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data type\n",
    "Images are 28x28 greyscale (MNIST description on Yann LeCunn's website)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialize DNN architecture\n",
    "# Input : layers =  [n_features, n1, n2, ... , n_output]\n",
    "\n",
    "def initialize_DNN(layers):\n",
    "    parameters = {}\n",
    "    for i in range(1, len(layers)):\n",
    "        parameters[\"W\" + str(i)] = np.random.randn(layers[i], layers[i-1]) * (1.0 / layers[i-1])\n",
    "        parameters[\"b\" + str(i)] = np.zeros((layers[i], 1))\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'b2': array([[ 0.],\n",
      "       [ 0.]]), 'b1': array([[ 0.],\n",
      "       [ 0.],\n",
      "       [ 0.]]), 'W1': array([[ 0.45813492,  0.50746729,  0.04959326,  0.49361363],\n",
      "       [ 0.01219896,  0.72663458, -0.2120326 , -0.09057736],\n",
      "       [ 0.3024943 , -0.23985896,  0.50528011,  0.17214653]]), 'W2': array([[-0.28361862, -0.05729338, -0.25124414],\n",
      "       [-0.01120927,  0.27350801,  0.11972458]])}\n"
     ]
    }
   ],
   "source": [
    "test_param = initialize_DNN([4,3,2])\n",
    "print(test_param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def apply_activation(Z, activation):\n",
    "    \n",
    "    if (activation == \"tanh\"):\n",
    "        return np.tanh(Z)\n",
    "    elif (activation == \"sigmoid\"):\n",
    "        return 1.0 / (1.0 + np.exp(-Z))\n",
    "    \n",
    "    #if activation == \"relu\":\n",
    "    #    return tf.nn.relu(Z)\n",
    "    #    #return Z * (Z > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Forward propagation\n",
    "\n",
    "def forward_propagation(X, layers, parameters, activation):\n",
    "    A_prev = X.T\n",
    "    cache = {}\n",
    "    n_layers = len(layers)\n",
    "    \n",
    "    for i in range(1, n_layers-1):\n",
    "        W = parameters[\"W\" + str(i)]\n",
    "        b = parameters[\"b\" + str(i)]\n",
    "        Z = np.dot(W,A_prev) + b \n",
    "        A = apply_activation(Z, activation = \"tanh\")\n",
    "        \n",
    "        cache[\"Z\" + str(i)] = Z\n",
    "        cache[\"A\" + str(i)] = A\n",
    "        \n",
    "        A_prev = A\n",
    "    \n",
    "    # Output layer\n",
    "    W = parameters[\"W\" + str(n_layers-1)]\n",
    "    b = parameters[\"b\" + str(n_layers-1)]\n",
    "    Z = np.dot(W,A_prev) + b \n",
    "    A = apply_activation(Z, activation = \"sigmoid\") # This should be a softmax\n",
    "        \n",
    "    cache[\"Z\" + str(n_layers-1)] = Z\n",
    "    cache[\"A\" + str(n_layers-1)] = A    \n",
    "        \n",
    "    return parameters, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'A1': array([[-0.00285962,  0.00337693],\n",
      "       [-0.00728551,  0.02315124],\n",
      "       [-0.01494057,  0.00721465],\n",
      "       [-0.02039987,  0.00262549],\n",
      "       [-0.0147092 , -0.00639248]]), 'A2': array([[ 0.50008659,  0.49830257],\n",
      "       [ 0.5007718 ,  0.49948983],\n",
      "       [ 0.50053388,  0.49888125],\n",
      "       [ 0.50090101,  0.50102691],\n",
      "       [ 0.49846668,  0.50348128],\n",
      "       [ 0.49967482,  0.49809519],\n",
      "       [ 0.50233387,  0.50018957],\n",
      "       [ 0.50001385,  0.49961647],\n",
      "       [ 0.50080949,  0.49945294],\n",
      "       [ 0.50072151,  0.50190094]]), 'Z1': array([[-0.00285963,  0.00337694],\n",
      "       [-0.00728564,  0.02315537],\n",
      "       [-0.01494168,  0.00721478],\n",
      "       [-0.0204027 ,  0.0026255 ],\n",
      "       [-0.01471026, -0.00639257]]), 'Z2': array([[  3.46370422e-04,  -6.78974384e-03],\n",
      "       [  3.08719368e-03,  -2.04066119e-03],\n",
      "       [  2.13553163e-03,  -4.47499638e-03],\n",
      "       [  3.60403306e-03,   4.10763561e-03],\n",
      "       [ -6.13329193e-03,   1.39253612e-02],\n",
      "       [ -1.30072608e-03,  -7.61926581e-03],\n",
      "       [  9.33555211e-03,   7.58290940e-04],\n",
      "       [  5.53999322e-05,  -1.53412574e-03],\n",
      "       [  3.23796254e-03,  -2.18825374e-03],\n",
      "       [  2.88602818e-03,   7.60379114e-03]])}\n"
     ]
    }
   ],
   "source": [
    "parameters = initialize_DNN(layers)\n",
    "\n",
    "_, cache = forward_propagation(x_explore, layers, parameters, activation=\"relu\")\n",
    "print(cache)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cost funtion\n",
    "\n",
    "def compute_cost(Y, Y_hat):\n",
    "    m = Y.shape[1]\n",
    "    \n",
    "    cost = - (1.0 / m) * np.sum((np.dot(Y.T, np.log(Y_hat)) + np.dot((1.0-Y).T, np.log((1.0-Y_hat)))))\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.8801984796\n"
     ]
    }
   ],
   "source": [
    "cost = compute_cost(y_explore.T, cache[\"A\" + str(len(layers)-1)])\n",
    "print(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (<ipython-input-131-6a2273b56cc5>, line 13)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-131-6a2273b56cc5>\"\u001b[0;36m, line \u001b[0;32m13\u001b[0m\n\u001b[0;31m    \u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "# Back Prop:\n",
    "\n",
    "def backward_propagation(Y, cache, parameters, layers):\n",
    "    m = Y.shape[1]\n",
    "    n_layers = len(layers)\n",
    "    grads = {}\n",
    "\n",
    "    # Derivative of a softmax??\n",
    "    grads[\"dZ\" + str(n_layers-1)] = Y - cache(\"A\" + str(n_layers-1))\n",
    "    \n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
